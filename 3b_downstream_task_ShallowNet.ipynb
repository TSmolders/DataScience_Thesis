{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIM: Fine-tune pretrained ShallowNet as multiclass classification model for predicting the psychiatric diagnosis based on SSL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch.nn as nn\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU, BatchNorm1d, Dropout, Dropout1d, ModuleList\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import lightning.pytorch as pl\n",
    "from torchmetrics import F1Score, ConfusionMatrix\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, TQDMProgressBar, RichProgressBar\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from optuna.visualization.matplotlib import plot_contour\n",
    "from optuna.visualization.matplotlib import plot_edf\n",
    "from optuna.visualization.matplotlib import plot_intermediate_values\n",
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "from optuna.visualization.matplotlib import plot_parallel_coordinate\n",
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "from optuna.visualization.matplotlib import plot_rank\n",
    "from optuna.visualization.matplotlib import plot_slice\n",
    "from optuna.visualization.matplotlib import plot_timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    with open(file, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "# functions for loading the epoched EEG data\n",
    "def get_filepath(epoch_dir, participant_ids):\n",
    "    \"\"\"\n",
    "    Function to get the filepath of the epoched EEG recording\n",
    "    :param epoch_dir: directory containing the epoched EEG recordings\n",
    "    :param participant_ids: list of participant IDs to include\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    found_ids = set()\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(epoch_dir):\n",
    "        for file in files:\n",
    "            for participant_id in participant_ids:\n",
    "                if participant_id in file:\n",
    "                    filepaths.append(os.path.join(subdir, file))\n",
    "                    found_ids.add(participant_id)\n",
    "    \n",
    "    # Print participant IDs if no files matching those IDs are found\n",
    "    for participant_id in participant_ids:\n",
    "        if participant_id not in found_ids:\n",
    "            print(f\"No files found for participant ID {participant_id}\")\n",
    "\n",
    "    return filepaths\n",
    "\n",
    "class EpochDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, participant_ids, epoch_dir, df_classes):\n",
    "        self.filepaths = get_filepath(epoch_dir, participant_ids)\n",
    "        self.epochs = []\n",
    "        self.participant_ids = []\n",
    "        self.diagnosis = []\n",
    "        self.df_classes = df_classes\n",
    "        self._load_data()\n",
    "        assert len(self.epochs) == len(self.participant_ids) == len(self.diagnosis)\n",
    "        print(f\"Loaded {len(self.epochs)} epochs\")\n",
    "\n",
    "    def _load_data(self):\n",
    "        all_epochs = []\n",
    "        for filepath in self.filepaths:\n",
    "            epochs = torch.load(filepath)\n",
    "            # get participant ID from filepath to make sure the participant ID is correct\n",
    "            participant_id = filepath.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "            # get the diagnosis of the participant\n",
    "            diagnosis = self.df_classes[self.df_classes[\"participants_ID\"] == participant_id][\"labels\"].values[0]\n",
    "\n",
    "            all_epochs.append(epochs)\n",
    "            self.participant_ids.extend([participant_id]*epochs.shape[0])\n",
    "            self.diagnosis.extend([diagnosis]*epochs.shape[0])\n",
    "\n",
    "        self.epochs = np.concatenate(all_epochs, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epochs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        epoch = self.epochs[idx]\n",
    "        y = self.diagnosis[idx]\n",
    "        participant_id = self.participant_ids[idx]\n",
    "        return torch.tensor(epoch, dtype=torch.float32), y, participant_id\n",
    "    \n",
    "def transfer_freeze_weights(pretrained_weights, model, freeze=True):\n",
    "            \"\"\"\n",
    "            Function to transfer the pretrained weights to the downstream model\n",
    "            param: pretrained_weights: the weights to transfer in a dictionary\n",
    "            param: model: the model to transfer the weights to\n",
    "            param: freeze: whether to freeze the pretrained weights or not\n",
    "            \"\"\"\n",
    "            pretrained_model = model\n",
    "            modified_keys = {}\n",
    "            for k, v in pretrained_weights.items():\n",
    "                decomposed_key = k.split('.')\n",
    "                if decomposed_key[0] == 'EEGNet' or decomposed_key[0] == 'ShallowNet': # remove the first part of each key to match the model's keys\n",
    "                    pretrained_key = '.'.join(decomposed_key[1:])\n",
    "                    modified_keys[pretrained_key] = v\n",
    "\n",
    "            pretrained_model.load_state_dict(modified_keys, strict=False)\n",
    "            if freeze: # freeze only the layers that have been pretrained\n",
    "                for name, param in pretrained_model.named_parameters():\n",
    "                    if name in modified_keys.keys():\n",
    "                        # print(f'Freezing layer: {name}')\n",
    "                        param.requires_grad = False\n",
    "            return pretrained_model\n",
    "\n",
    "def evaluate_best_model(test_scores, best_outer_hyperparameters, checkpoints_path):\n",
    "    \"\"\"\n",
    "    Function to evaluate the best model based on the test scores\n",
    "    :param test_scores: the test scores for each fold\n",
    "    :param best_outer_hyperparameters: the best hyperparameters for each fold\n",
    "    \"\"\"\n",
    "    best_fold = np.argmax(test_scores)\n",
    "    best_params = best_outer_hyperparameters[best_fold]\n",
    "    print(f'Best fold: {best_fold}')\n",
    "    print(f'Best hyperparameters: {best_params}')\n",
    "    print(f'Best test f1-score: {test_scores[best_fold]}')\n",
    "\n",
    "    # load the best model\n",
    "    fc_layers=best_params['fc_layers']\n",
    "    dropout=best_params['dropout']\n",
    "    optimizer_name=best_params['optimizer']\n",
    "    learning_rate=best_params['learning_rate']\n",
    "\n",
    "    best_model_path = os.path.join(checkpoints_path, f'fold-{best_fold}-last.ckpt')\n",
    "    print(f\"Best model checkpoint path: {best_model_path}\")\n",
    "    best_model = ShallowNet.load_from_checkpoint(\n",
    "        checkpoint_path=best_model_path,\n",
    "        fc_layers=fc_layers,\n",
    "        dropout=dropout,\n",
    "        optimizer_name=optimizer_name,\n",
    "        learning_rate=learning_rate,\n",
    "    )\n",
    "\n",
    "    # obtain classification report and confusion matrix on all data\n",
    "    dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    trainer = Trainer(accelerator='gpu', devices=1, enable_progress_bar=False, enable_model_summary=False, logger=False, enable_checkpointing=False)\n",
    "    y_hat = trainer.predict(best_model, dataloaders=dataloader)\n",
    "    y_true = np.array(dataset.diagnosis)\n",
    "    y_hat = np.concatenate(y_hat).flatten()\n",
    "    print(y_hat.shape)\n",
    "    print(classification_report(y_true, y_hat, target_names=le.classes_))\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot the first confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_hat)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "    disp.plot(cmap='Blues', values_format='d', colorbar=False, ax=axes[0])\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "    # Plot the normalized confusion matrix\n",
    "    cm_norm = confusion_matrix(y_true, y_hat, normalize='true')\n",
    "    disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=le.classes_)\n",
    "    disp_norm.plot(cmap='Blues', colorbar=False, ax=axes[1])\n",
    "    axes[1].set_title('Normalized Confusion Matrix')\n",
    "    axes[1].grid(False)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_optuna(n_epochs, n_trials, dataset, labels, groups, pretrained_weights):\n",
    "    '''\t\n",
    "    Function to perform nested cross-validation with hyperparameter optimization for ShallowNet using Optuna\n",
    "    :param n_epochs: number of epochs to train the model\n",
    "    :param n_trials: number of trials for the hyperparameter optimization\n",
    "    :param dataset: the dataset containing the EEG epochs\n",
    "    :param labels: the labels of the dataset\n",
    "    :param groups: the participant IDs of the dataset\n",
    "    :param pretrained_weights: the pretrained weights to transfer to the model\n",
    "    :return: the test scores and the best hyperparameters for each fold\n",
    "    '''\n",
    "    outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "    test_scores = []\n",
    "    best_outer_hyperparameters = {}\n",
    "    logger = CSVLogger(\"logs\", name='ShallowNetTest') # log results to csv file\n",
    "    torch.set_float32_matmul_precision('high') # for use of tensor cores\n",
    "    for i, (train_val_idx, test_idx) in enumerate(outer_cv.split(dataset, labels, groups)):\n",
    "        # print(f'Size of train/val set: {len(train_val_idx)}')\n",
    "        # print(f'Size of test set: {len(test_idx)}')\n",
    "        train_val_set = Subset(dataset, train_val_idx)\n",
    "        test_set = Subset(dataset, test_idx)\n",
    "        \n",
    "        # inner cv\n",
    "        def objective_cv(trial):\n",
    "            # suggest values for the hyperparameters\n",
    "            learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "            fc_layers = trial.suggest_int('fc_layers', 1, 5)\n",
    "            optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop', 'SGD'])\n",
    "            batch_size = trial.suggest_categorical('batch_size', [50, 100, 178, 350, 476, 700])\n",
    "            dropout = trial.suggest_float('dropout', 0, 0.8)\n",
    "            hyperparameters = dict(learning_rate=learning_rate, fc_layers=fc_layers, optimizer_name=optimizer_name,\n",
    "                                batch_size=batch_size, dropout=dropout)\n",
    "            \n",
    "            # n_epochs = batch_size * 4 # vary training epochs based on batch size\n",
    "\n",
    "            # create the model\n",
    "            model = ShallowNet(fc_layers=fc_layers, dropout=dropout, optimizer_name=optimizer_name, learning_rate=learning_rate)\n",
    "\n",
    "            pretrained_model = transfer_freeze_weights(pretrained_weights, model, freeze=True)\n",
    "            \n",
    "            # inner cv\n",
    "            inner_cv = StratifiedGroupKFold(n_splits=3, shuffle=False)\n",
    "            val_scores = []\n",
    "            for j, (train_idx, val_idx) in enumerate(inner_cv.split(train_val_set, labels[train_val_idx], groups[train_val_idx])):\n",
    "                # print(f'Size of train set: {len(train_idx)}')\n",
    "                # print(f'Size of val set: {len(val_idx)}')\n",
    "                train_set = Subset(train_val_set, train_idx)\n",
    "                val_set = Subset(train_val_set, val_idx)\n",
    "                # Redirect both stdout and stderr\n",
    "                with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
    "                    trainer = Trainer(\n",
    "                        accelerator='gpu',\n",
    "                        devices=1,\n",
    "                        max_epochs=n_epochs,\n",
    "                        enable_progress_bar=False,  # Disable progress bar\n",
    "                        enable_model_summary=False,\n",
    "                        logger=False,  # Disable logging\n",
    "                        enable_checkpointing=False,\n",
    "                    )\n",
    "\n",
    "                    # Train model\n",
    "                    trainer.fit(pretrained_model, train_dataloaders=DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "                                val_dataloaders=DataLoader(val_set, batch_size=batch_size, shuffle=False))\n",
    "\n",
    "                    # Evaluate the model on the validation set\n",
    "                    val_result = trainer.validate(pretrained_model, dataloaders=DataLoader(val_set, batch_size=len(val_set), shuffle=False))\n",
    "                # print(val_result)\n",
    "                val_scores.append(val_result[0]['val_f1'])  # Optimizing based on validation f1-score\n",
    "            \n",
    "            # return the average validation score\n",
    "            avg_val_f1 = np.mean(val_scores)\n",
    "            return avg_val_f1\n",
    "\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective_cv, n_trials=n_trials)\n",
    "\n",
    "        # print(f'Best hyperparameters for fold {i}: {study.best_params}')\n",
    "        best_outer_hyperparameters[i] = study.best_params\n",
    "\n",
    "        # train the best inner model on the entire train/val set with the best hyperparameters\n",
    "        best_params = study.best_params\n",
    "        final_model = ShallowNet(\n",
    "            fc_layers=best_params['fc_layers'],\n",
    "            dropout=best_params['dropout'],\n",
    "            optimizer_name=best_params['optimizer'],\n",
    "            learning_rate=best_params['learning_rate']\n",
    "        )\n",
    "\n",
    "        pretrained_final_model = transfer_freeze_weights(pretrained_weights, final_model, freeze=True)\n",
    "        \n",
    "        # Define the checkpoint callback with the fold number in the filename\n",
    "        fully_trained_callback = ModelCheckpoint(\n",
    "            filename=f'fold-{i}-last'\n",
    "        )\n",
    "\n",
    "        final_trainer = Trainer(\n",
    "            accelerator='gpu',\n",
    "            devices=1,\n",
    "            max_epochs=n_epochs,\n",
    "            enable_progress_bar=False,\n",
    "            enable_model_summary=False,\n",
    "            logger=logger,\n",
    "            enable_checkpointing=True,\n",
    "            callbacks=[fully_trained_callback],\n",
    "        )\n",
    "        final_trainer.fit(\n",
    "            pretrained_final_model,\n",
    "            train_dataloaders=DataLoader(train_val_set, batch_size=best_params['batch_size'], shuffle=True), # maybe drop_last=True??\n",
    "        )\n",
    "\n",
    "        # evaluate the model on the test set\n",
    "        test_result = final_trainer.test(pretrained_final_model, dataloaders=DataLoader(test_set, batch_size=len(test_set), shuffle=False))\n",
    "        print(f'Test result for fold {i}: {test_result}')\n",
    "        test_scores.append(test_result[0]['test_f1'])\n",
    "    \n",
    "    return test_scores, best_outer_hyperparameters, fully_trained_callback.dirpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_tuning(n_epochs, n_trials, dataset, pretext_task, weights_dir=r'D:\\Documents\\Master_Data_Science\\Thesis\\thesis_code\\DataScience_Thesis\\data\\pretext_model_weights'):\n",
    "    '''\n",
    "    Function to perform the pipeline for hyperparameter tuning\n",
    "    :param n_epochs: number of epochs to train the model\n",
    "    :param n_trials: number of trials for the hyperparameter optimization\n",
    "    :param dataset: the dataset containing the EEG epochs\n",
    "    :param: pretext_task: a string indicating the specific pretext task to load the weights from\n",
    "    :param weights_dir: the directory containing the pretrained weights\n",
    "    '''\n",
    "    labels = np.array(dataset.diagnosis)\n",
    "    groups = np.array(dataset.participant_ids)\n",
    "    pretrained_weights = torch.load(f'{weights_dir}\\{pretext_task}_weights.pt')\n",
    "    test_scores, best_outer_hyperparameters, checkpoints_path = nested_cv_optuna(n_epochs, n_trials, dataset, labels, groups, pretrained_weights)\n",
    "    return test_scores, best_outer_hyperparameters, checkpoints_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShallowNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNet(pl.LightningModule):\n",
    "        \"\"\"\n",
    "        Pytorch implementation of the ShallowNet Encoder.\n",
    "        Code taken and adjusted from:\n",
    "        https://github.com/MedMaxLab/selfEEG/blob/024402ba4bde95051d86ab2524cc71105bfd5c25/selfeeg/models/zoo.py#L693\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    fc_layers, # number of fully connected layers\n",
    "                    learning_rate, # learning rate\n",
    "                    optimizer_name, # optimizer\n",
    "                    dropout, # dropout probability\n",
    "                    num_classes=5, # number of classes in the dataset\n",
    "                    samples=1244,\n",
    "                    chans=26, # number of EEG channels\n",
    "                    F=40, # number of output filters in the temporal convolution layer\n",
    "                    K1=25, # length of the temporal convolutional layer\n",
    "                    pool=75, # temporal pooling kernel size\n",
    "                    num_extracted_features=100, # number of features to extract\n",
    "                    ):\n",
    "            super(ShallowNet, self).__init__()\n",
    "            self.learning_rate = learning_rate\n",
    "            self.optimizer_name = optimizer_name\n",
    "\n",
    "            self.conv1 = nn.Conv2d(1, F, (1, K1), stride=(1, 1))\n",
    "            self.conv2 = nn.Conv2d(F, F, (chans, 1), stride=(1, 1))\n",
    "            self.batch1 = nn.BatchNorm2d(F)\n",
    "            self.pool2 = nn.AvgPool2d((1, pool), stride=(1, 15))\n",
    "            self.flatten2 = nn.Flatten()\n",
    "            self.drop1 = nn.Dropout(0.2)\n",
    "            self.lin = nn.Linear(\n",
    "                F * ((samples - K1 + 1 - pool) // 15 + 1), num_extracted_features\n",
    "            )\n",
    "\n",
    "            # part for downstream task\n",
    "            self.batch2 = nn.BatchNorm1d(num_extracted_features)\n",
    "            self.fcs = ModuleList()\n",
    "            self.dropouts = ModuleList()\n",
    "            input_size = num_extracted_features # Input size for the first layer\n",
    "            output_size = num_classes # Output size for the last layer\n",
    "            sizes = np.linspace(input_size, output_size, fc_layers + 1, dtype=int) # Calculate the size for each layer\n",
    "            for i in range(fc_layers): # Create the layers\n",
    "                self.fcs.append(Linear(sizes[i], sizes[i + 1]))\n",
    "                if i < fc_layers - 1:  # Append dropout only if it's not the last layer\n",
    "                    self.dropouts.append(Dropout(p=dropout))\n",
    "\n",
    "            # add metrics\n",
    "            self.train_f1 = F1Score(task='multiclass', num_classes=5, average='macro')\n",
    "            self.val_f1 = F1Score(task='multiclass', num_classes=5, average='macro')\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.batch1(x)\n",
    "            x = torch.square(x)\n",
    "            x = self.pool2(x)\n",
    "            x = torch.log(torch.clamp(x, 1e-7, 10000))\n",
    "            x = self.flatten2(x)\n",
    "            x = self.drop1(x)\n",
    "            x = self.lin(x) # These are the SSL features\n",
    "            x = self.batch2(x) # Standardize the features\n",
    "            for i, fc in enumerate(self.fcs):\n",
    "                if i < len(self.fcs) - 1:  # Apply relu only if it's not the last layer\n",
    "                    x = F.relu(fc(x))\n",
    "                    x = self.dropouts[i](x)\n",
    "                else:\n",
    "                    x = fc(x) # Apply the last layer without relu & no dropout\n",
    "\n",
    "            return x\n",
    "        \n",
    "        def configure_optimizers(self):\n",
    "            if self.optimizer_name == 'Adam':\n",
    "                optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "            elif self.optimizer_name == 'RMSprop':\n",
    "                optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
    "            elif self.optimizer_name == 'SGD':\n",
    "                optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "            else:\n",
    "                raise ValueError(f'Unsupported optimizer: {self.optimizer_name}')\n",
    "            return optimizer\n",
    "        \n",
    "        def training_step(self, train_batch, batch_idx):\n",
    "            x = train_batch[0]\n",
    "            y = train_batch[1]\n",
    "            x = x.unsqueeze(1)\n",
    "            y = y.long()\n",
    "            logits = self.forward(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            y_hat = torch.argmax(logits, dim=1)\n",
    "            self.log('train_loss', loss, on_epoch=True, prog_bar=True)\n",
    "            self.log('train_f1', self.train_f1(y_hat, y), on_epoch=True, prog_bar=True)\n",
    "            return loss\n",
    "        \n",
    "        def validation_step(self, val_batch, batch_idx):\n",
    "            x = val_batch[0]\n",
    "            y = val_batch[1]\n",
    "            x = x.unsqueeze(1)\n",
    "            y = y.long()\n",
    "            logits = self.forward(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            y_hat = torch.argmax(logits, dim=1)\n",
    "            self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "            self.log('val_f1', self.val_f1(y_hat, y), on_epoch=True, prog_bar=True)\n",
    "            return loss\n",
    "        \n",
    "        def test_step(self, test_batch, batch_idx):\n",
    "            x = test_batch[0]\n",
    "            y = test_batch[1]\n",
    "            x = x.unsqueeze(1)\n",
    "            y = y.long()\n",
    "            logits = self.forward(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            y_hat = torch.argmax(logits, dim=1)\n",
    "            self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "            self.log('test_f1', self.val_f1(y_hat, y), on_epoch=True, prog_bar=True)\n",
    "            return loss\n",
    "        \n",
    "        def predict_step(self, batch):\n",
    "            x = batch[0]\n",
    "            x = x.unsqueeze(1)\n",
    "            logits = self.forward(x)\n",
    "            y_hat = torch.argmax(logits, dim=1)\n",
    "            return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 13)\n",
      "diagnosis\n",
      "ADHD       45\n",
      "HEALTHY    45\n",
      "MDD        45\n",
      "OCD        45\n",
      "SMC        45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_participants = pd.read_pickle(r'D:\\Documents\\RU\\Master_Neurobiology\\Internship_jaar_2\\Project\\TD-BRAIN\\TDBRAIN_participants_V2_data\\df_participants.pkl')\n",
    "sample_df = pd.read_pickle(r'D:\\Documents\\RU\\Master_Neurobiology\\Internship_jaar_2\\Project\\TD-BRAIN\\TD-BRAIN_extracted_features\\df_selected_stat_features.pkl')\n",
    "sample_ids = sample_df['ID'].unique() # obtain unique IDs from subsampled dataframe containing epoched features\n",
    "df_sample = df_participants[df_participants['participants_ID'].isin(sample_ids)] # filter participants dataframe to only include subsampled IDs\n",
    "df_sample = df_sample[df_sample['sessID'] == 1] # filter first session\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df_sample['diagnosis'])\n",
    "df_sample['labels'] = le.transform(df_sample['diagnosis'])\n",
    "# \n",
    "print(df_sample.shape)\n",
    "print(df_sample['diagnosis'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for participant ID sub-88073521\n",
      "Loaded 2688 epochs\n",
      "2688\n",
      "torch.Size([26, 1244])\n",
      "4\n",
      "sub-87964717\n"
     ]
    }
   ],
   "source": [
    "# load the epochs into a dataset\n",
    "participant_ids = df_sample['participants_ID'].tolist()\n",
    "dataset = EpochDataset(participant_ids, r\"D:\\Documents\\RU\\Master_Neurobiology\\Internship_jaar_2\\Project\\TD-BRAIN\\TDBRAIN-dataset-derivatives\\thesis_epoched_data\\EC\",\n",
    "                       df_sample)\n",
    "print(len(dataset))\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1])\n",
    "print(dataset[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Within RP pretext task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Tpos = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture # cannot suppress output from pytorch lightning\n",
    "# perform nested cross-validation with hyperparameter optimization\n",
    "test_scores, best_outer_hyperparameters, checkpoints_path = pipeline_tuning(\n",
    "    n_epochs=3,\n",
    "    n_trials=1,\n",
    "    dataset=dataset,\n",
    "    pretext_task='fullytrained_withinRP_tpos1_ShallowNet_pretext_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate tuning results\n",
    "print(f'Test f1-scores: {test_scores}')\n",
    "print(f'Average test f1-scores: {np.mean(test_scores)}')\n",
    "print(f'Std of test f1-scores: {np.std(test_scores)}')\n",
    "print(f'Best fold test f1-score: {np.max(test_scores)}')\n",
    "\n",
    "evaluate_best_model(test_scores, best_outer_hyperparameters, checkpoints_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Tpos = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture # cannot suppress output from pytorch lightning\n",
    "# perform nested cross-validation with hyperparameter optimization\n",
    "test_scores, best_outer_hyperparameters, checkpoints_path = pipeline_tuning(\n",
    "    n_epochs=3,\n",
    "    n_trials=1,\n",
    "    dataset=dataset,\n",
    "    pretext_task='fullytrained_withinRP_tpos2_ShallowNet_pretext_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate tuning results\n",
    "print(f'Test f1-scores: {test_scores}')\n",
    "print(f'Average test f1-scores: {np.mean(test_scores)}')\n",
    "print(f'Std of test f1-scores: {np.std(test_scores)}')\n",
    "print(f'Best fold test f1-score: {np.max(test_scores)}')\n",
    "\n",
    "evaluate_best_model(test_scores, best_outer_hyperparameters, checkpoints_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Tpos = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture # cannot suppress output from pytorch lightning\n",
    "# perform nested cross-validation with hyperparameter optimization\n",
    "test_scores, best_outer_hyperparameters, checkpoints_path = pipeline_tuning(\n",
    "    n_epochs=3,\n",
    "    n_trials=1,\n",
    "    dataset=dataset,\n",
    "    pretext_task='fullytrained_withinRP_tpos4_ShallowNet_pretext_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate tuning results\n",
    "print(f'Test f1-scores: {test_scores}')\n",
    "print(f'Average test f1-scores: {np.mean(test_scores)}')\n",
    "print(f'Std of test f1-scores: {np.std(test_scores)}')\n",
    "print(f'Best fold test f1-score: {np.max(test_scores)}')\n",
    "\n",
    "evaluate_best_model(test_scores, best_outer_hyperparameters, checkpoints_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Across RP pretext task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Tpos = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Tpos = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. Tpos = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Across Subject pretext task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contrastive loss pretext task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
