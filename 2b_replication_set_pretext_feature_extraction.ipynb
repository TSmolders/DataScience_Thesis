{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM: Extract features from the TDBRAIN replication set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now only extracted for the best models based on downstream performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchmetrics import F1Score, Accuracy\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# prevent extensive logging\n",
    "mne.set_log_level('WARNING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading epoch data & participant data of replication sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participants_ID</th>\n",
       "      <th>DISC/REP</th>\n",
       "      <th>indication</th>\n",
       "      <th>formal_status</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Consent</th>\n",
       "      <th>sessSeason</th>\n",
       "      <th>sessTime</th>\n",
       "      <th>Responder</th>\n",
       "      <th>Remitter</th>\n",
       "      <th>...</th>\n",
       "      <th>BDI_pre</th>\n",
       "      <th>BDI_post</th>\n",
       "      <th>rTMS PROTOCOL</th>\n",
       "      <th>ADHD_pre_Hyp_leading</th>\n",
       "      <th>ADHD_pre_Att_leading</th>\n",
       "      <th>ADHD_post_Att_leading</th>\n",
       "      <th>ADHD_post_Hyp_leading</th>\n",
       "      <th>NF Protocol</th>\n",
       "      <th>YBOCS_pre</th>\n",
       "      <th>YBOCS_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19681349</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>2.0</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19681385</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>1.0</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19684666</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>1.0</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19686324</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>2.0</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19687321</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>3.0</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>19739349</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>19739424</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>19740051</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>8.0</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>19740274</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>19741824</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>YES</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>...</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>7.0</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "      <td>REPLICATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participants_ID     DISC/REP   indication formal_status      Dataset  \\\n",
       "0          19681349  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "1          19681385  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "2          19684666  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "3          19686324  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "4          19687321  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "..              ...          ...          ...           ...          ...   \n",
       "115        19739349  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "116        19739424  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "117        19740051  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "118        19740274  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "119        19741824  REPLICATION  REPLICATION   REPLICATION  REPLICATION   \n",
       "\n",
       "    Consent   sessSeason     sessTime    Responder     Remitter  ...  \\\n",
       "0       YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "1       YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "2       YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "3       YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "4       YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "..      ...          ...          ...          ...          ...  ...   \n",
       "115     YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "116     YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "117     YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "118     YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "119     YES  REPLICATION  REPLICATION  REPLICATION  REPLICATION  ...   \n",
       "\n",
       "         BDI_pre     BDI_post  rTMS PROTOCOL  ADHD_pre_Hyp_leading  \\\n",
       "0    REPLICATION  REPLICATION            2.0           REPLICATION   \n",
       "1    REPLICATION  REPLICATION            1.0           REPLICATION   \n",
       "2    REPLICATION  REPLICATION            1.0           REPLICATION   \n",
       "3    REPLICATION  REPLICATION            2.0           REPLICATION   \n",
       "4    REPLICATION  REPLICATION            3.0           REPLICATION   \n",
       "..           ...          ...            ...                   ...   \n",
       "115  REPLICATION  REPLICATION            NaN           REPLICATION   \n",
       "116  REPLICATION  REPLICATION            NaN           REPLICATION   \n",
       "117  REPLICATION  REPLICATION            8.0           REPLICATION   \n",
       "118  REPLICATION  REPLICATION            NaN           REPLICATION   \n",
       "119  REPLICATION  REPLICATION            7.0           REPLICATION   \n",
       "\n",
       "     ADHD_pre_Att_leading  ADHD_post_Att_leading ADHD_post_Hyp_leading  \\\n",
       "0             REPLICATION            REPLICATION           REPLICATION   \n",
       "1             REPLICATION            REPLICATION           REPLICATION   \n",
       "2             REPLICATION            REPLICATION           REPLICATION   \n",
       "3             REPLICATION            REPLICATION           REPLICATION   \n",
       "4             REPLICATION            REPLICATION           REPLICATION   \n",
       "..                    ...                    ...                   ...   \n",
       "115           REPLICATION            REPLICATION           REPLICATION   \n",
       "116           REPLICATION            REPLICATION           REPLICATION   \n",
       "117           REPLICATION            REPLICATION           REPLICATION   \n",
       "118           REPLICATION            REPLICATION           REPLICATION   \n",
       "119           REPLICATION            REPLICATION           REPLICATION   \n",
       "\n",
       "     NF Protocol    YBOCS_pre   YBOCS_post  \n",
       "0    REPLICATION  REPLICATION  REPLICATION  \n",
       "1    REPLICATION  REPLICATION  REPLICATION  \n",
       "2    REPLICATION  REPLICATION  REPLICATION  \n",
       "3    REPLICATION  REPLICATION  REPLICATION  \n",
       "4    REPLICATION  REPLICATION  REPLICATION  \n",
       "..           ...          ...          ...  \n",
       "115  REPLICATION  REPLICATION  REPLICATION  \n",
       "116  REPLICATION  REPLICATION  REPLICATION  \n",
       "117  REPLICATION  REPLICATION  REPLICATION  \n",
       "118  REPLICATION  REPLICATION  REPLICATION  \n",
       "119  REPLICATION  REPLICATION  REPLICATION  \n",
       "\n",
       "[120 rows x 110 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_replication = pd.read_csv(r'D:\\Documents\\RU\\Master_Neurobiology\\Internship_jaar_2\\Project\\TD-BRAIN\\TDBRAIN_participants_V2_data\\TDBRAIN_replication_template_V2.csv')\n",
    "# set participants_ID column as string\n",
    "df_replication['participants_ID'] = df_replication['participants_ID'].astype(str)\n",
    "df_replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for loading the epoched EEG data\n",
    "def get_filepath(epoch_dir, participant_ids):\n",
    "    \"\"\"\n",
    "    Function to get the filepath of the epoched EEG recording\n",
    "    :param epoch_dir: directory containing the epoched EEG recordings\n",
    "    :param participant_ids: list of participant IDs to include\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    found_ids = set()\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(epoch_dir):\n",
    "        for file in files:\n",
    "            for participant_id in participant_ids:\n",
    "                if participant_id in file:\n",
    "                    filepaths.append(os.path.join(subdir, file))\n",
    "                    found_ids.add(participant_id)\n",
    "    \n",
    "    # Print participant IDs if no files matching those IDs are found\n",
    "    for participant_id in participant_ids:\n",
    "        if participant_id not in found_ids:\n",
    "            print(f\"No files found for participant ID {participant_id}\")\n",
    "\n",
    "    return filepaths\n",
    "\n",
    "class EpochDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, participant_ids, epoch_dir):\n",
    "        self.filepaths = get_filepath(epoch_dir, participant_ids)\n",
    "        self.participant_ids = participant_ids\n",
    "        self.epochs = []\n",
    "        self.participant_ids = []\n",
    "        self._load_data()\n",
    "        print(f\"Number of epochs: {self.epochs.shape[0]}\")\n",
    "        print(f\"Number of participants: {len(self.participant_ids)}\")\n",
    "\n",
    "    def _load_data(self):\n",
    "        all_epochs = []\n",
    "        for filepath in self.filepaths:\n",
    "            epochs = torch.load(filepath)\n",
    "            # get participant ID from filepath to make sure the participant ID is correct\n",
    "            participant_id = filepath.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "            all_epochs.append(epochs)\n",
    "            self.participant_ids.extend([participant_id]*epochs.shape[0])\n",
    "        self.epochs = np.concatenate(all_epochs, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epochs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        epoch = self.epochs[idx]\n",
    "        participant_id = self.participant_ids[idx]\n",
    "        return torch.tensor(epoch, dtype=torch.float32), participant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 1440\n",
      "Number of participants: 1440\n",
      "1440\n",
      "torch.Size([26, 1244])\n",
      "sub-19681349\n",
      "sub-19681349\n"
     ]
    }
   ],
   "source": [
    "# load the epochs into a dataset\n",
    "participant_ids = df_replication['participants_ID'].tolist()\n",
    "dataset = EpochDataset(participant_ids, r\"D:\\Documents\\RU\\Master_Neurobiology\\Internship_jaar_2\\Project\\TD-BRAIN\\TDBRAIN-dataset-derivatives\\thesis_epoched_data\\EC\")\n",
    "print(len(dataset))\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1])\n",
    "print(dataset[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfering pretrained weights & extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions have been adjusted specifically for the replication set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights(pretrained_weights, pretext_model):\n",
    "    \"\"\"\n",
    "    Function to transfer the pretrained weights to the pretext model\n",
    "    param: pretrained_weights: the weights to transfer in a dictionary\n",
    "    param: pretext_model: the model to transfer the weights to\n",
    "    \"\"\"\n",
    "    pretrained_model = pretext_model\n",
    "    modified_keys = {}\n",
    "    for k, v in pretrained_weights.items():\n",
    "        decomposed_key = k.split('.')\n",
    "        if decomposed_key[0] == 'EEGNet' or decomposed_key[0] == 'ShallowNet': # remove the first part of each key to match the model's keys\n",
    "            pretrained_key = '.'.join(decomposed_key[1:])\n",
    "            modified_keys[pretrained_key] = v\n",
    "\n",
    "            \n",
    "    pretrained_model.load_state_dict(modified_keys)\n",
    "        \n",
    "    return pretrained_model\n",
    "\n",
    "def extract_features(pretrained_model, data, pretext_task, df_sample, to_disk=False, num_extracted_features=100, add_missing_ids=False):\n",
    "    \"\"\"\n",
    "    Function to extract features from the pretrained model\n",
    "    param: pretrained_model: the model to extract features from\n",
    "    param: data: the dataset containing the epochs to extract features from\n",
    "    param: pretext_task: a string indicating the specific pretext task to save the features as\n",
    "    param: df_sample: the dataframe containing the sampled participant IDs and their corresponding diagnosis\n",
    "    param: to_disk: boolean to save the features to disk\n",
    "    param: num_extracted_features: the number of features to extract\n",
    "    param: add_missing_ids: boolean to add missing participant IDs to the dataframe\n",
    "    \"\"\"\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=1, shuffle=False)\n",
    "    pretrained_model.eval()\n",
    "    features_list = []\n",
    "    participant_ids = []\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in dataloader:\n",
    "            epoch, participant_id = batch  # Remove the batch dimension\n",
    "            epoch = epoch.unsqueeze(0)  # Add dimension\n",
    "            # print(epoch.shape)\n",
    "            features = pretrained_model(epoch)  # Extract features\n",
    "            features = features.squeeze(0)\n",
    "            features = features.numpy()\n",
    "            features_list.append(features)\n",
    "            participant_ids.append(participant_id[0])\n",
    "\n",
    "    if add_missing_ids:\n",
    "        # Probably not the best idea to add missing IDs with NaN values (in this\n",
    "        # case only one ID), but the baselines from the previous internetship were \n",
    "        # trained with the same approach (using mean impute for the 12 missing epochs of one participant). \n",
    "        # Considering only 1 participant is missing (due to 1 BAD preprocessing file), \n",
    "        # it shouldn't have a large impact on the results. But it's something to keep in mind for future work.\n",
    "\n",
    "        # check if any IDs are missing compared to the sampled IDs\n",
    "        missing_ids = set(df_sample['participants_ID'].tolist()) - set(participant_ids)\n",
    "        # if they are missing, add them with NaN values\n",
    "        if missing_ids:\n",
    "            for missing_id in missing_ids:\n",
    "                num_missing_epochs = 12  # Assuming 12 missing epochs for each missing participant\n",
    "                for _ in range(num_missing_epochs):\n",
    "                    features_list.append([np.nan] * num_extracted_features)\n",
    "                    participant_ids.append(missing_id)\n",
    "\n",
    "    features_df = pd.DataFrame(features_list) # store as dataframe\n",
    "    features_df['ID'] = participant_ids # add participant IDs to the dataframe\n",
    "\n",
    "    \n",
    "    print(f'{features_df.shape = }')\n",
    "    # print(f'{features_df[\"diagnosis\"].value_counts()}')\n",
    "    # print(f'{features_df.isnull().sum()}')\n",
    "    display(features_df.head(3))\n",
    "\n",
    "\n",
    "    if to_disk:\n",
    "        features_df.to_pickle(f'D:/Documents/Master_Data_Science/Thesis/thesis_code/DataScience_Thesis/data/SSL_features/df_replication_set_{pretext_task}_features.pkl')\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "def evaluate_features(features_df):\n",
    "    \"\"\"\n",
    "    Function to quickly evaluate the extracted features. Doesn't stratify/group data splitting! \n",
    "    \"\"\"\n",
    "    groups = features_df['ID']\n",
    "    X = features_df.drop(['ID', 'diagnosis'], axis=1)\n",
    "    y = features_df['diagnosis']\n",
    "\n",
    "    # if X contains NaN values, impute them with the feature-wise mean\n",
    "    if X.isnull().values.any():\n",
    "        X = X.fillna(X.mean())\n",
    "\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Get the train and test indices\n",
    "    for train_index, test_index in sgkf.split(X, y, groups):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        break  # Only use the first split\n",
    "    \n",
    "    # quick SVM model\n",
    "    clf = SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('quick SVM model')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f1_score(y_test, y_pred, average='macro'))\n",
    "    print()\n",
    "\n",
    "    # quick random forest model\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('quick random forest model')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "    return\n",
    "\n",
    "def get_ssl_features(\n",
    "        pretext_task,\n",
    "        data,\n",
    "        df_sample,\n",
    "        num_extracted_features=100,\n",
    "        eval=True,\n",
    "        to_disk=True,\n",
    "        pretext_model='EEGNet',\n",
    "        weights_dir=r'D:\\Documents\\Master_Data_Science\\Thesis\\thesis_code\\DataScience_Thesis\\data\\pretext_model_weights'\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Obtains SSL features from the weights trained by the pretext model.\n",
    "    param: pretext_task: a string indicating the specific pretext task to load the weights from and save the features as\n",
    "    param: data: the dataset containing the epochs to extract features from\n",
    "    param: df_sample: the dataframe containing the sampled participant IDs and their corresponding diagnosis\n",
    "    param: num_extracted_features: the number of features to extract\n",
    "    param: eval: boolean to evaluate the features\n",
    "    param: to_disk: boolean to save the features to disk\n",
    "    param: pretext_model: the model to extract features from\n",
    "    param: weights_dir: the directory containing the weights of the pretext model\n",
    "    \"\"\"\n",
    "    # Need to define model class here to avoid issues with different number of extracted features\n",
    "    # create Conv2d with max norm constraint\n",
    "    class Conv2dWithConstraint(nn.Conv2d):\n",
    "        def __init__(self, *args, max_norm: int = 1, **kwargs):\n",
    "            self.max_norm = max_norm\n",
    "            super(Conv2dWithConstraint, self).__init__(*args, **kwargs)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            self.weight.data = torch.renorm(self.weight.data, p=2, dim=0, maxnorm=self.max_norm)\n",
    "            return super(Conv2dWithConstraint, self).forward(x)\n",
    "        \n",
    "    class EEGNet(nn.Module):\n",
    "        \"\"\"\n",
    "        Code taken and adjusted from pytorch implementation of EEGNet\n",
    "        url: https://github.com/torcheeg/torcheeg/blob/v1.1.0/torcheeg/models/cnn/eegnet.py#L5\n",
    "        \"\"\"\n",
    "        def __init__(self,\n",
    "                    chunk_size: int = 1244, # number of data points in each EEG chunk\n",
    "                    num_electrodes: int = 26, # number of EEG electrodes\n",
    "                    F1: int = 8, # number of filters in first convolutional layer\n",
    "                    F2: int = 16, # number of filters in second convolutional layer\n",
    "                    D: int = 2, # depth multiplier\n",
    "                    num_extracted_features: int = num_extracted_features, # number of features to extract\n",
    "                    kernel_1: int = 64, # the filter size of block 1 (half of sfreq (125 Hz))\n",
    "                    kernel_2: int = 16, # the filter size of block 2 (one eight of sfreq (500 Hz))\n",
    "                    dropout: float = 0.25): # dropout rate\n",
    "            super(EEGNet, self).__init__()\n",
    "            self.F1 = F1\n",
    "            self.F2 = F2\n",
    "            self.D = D\n",
    "            self.chunk_size = chunk_size\n",
    "            self.num_extracted_features = num_extracted_features\n",
    "            self.num_electrodes = num_electrodes\n",
    "            self.kernel_1 = kernel_1\n",
    "            self.kernel_2 = kernel_2\n",
    "            self.dropout = dropout\n",
    "\n",
    "            self.block1 = nn.Sequential(\n",
    "                nn.Conv2d(1, self.F1, (1, self.kernel_1), stride=1, padding=(0, self.kernel_1 // 2), bias=False),\n",
    "                nn.BatchNorm2d(self.F1, momentum=0.01, affine=True, eps=1e-3),\n",
    "                Conv2dWithConstraint(self.F1,\n",
    "                                    self.F1 * self.D, (self.num_electrodes, 1),\n",
    "                                    max_norm=1,\n",
    "                                    stride=1,\n",
    "                                    padding=(0, 0),\n",
    "                                    groups=self.F1,\n",
    "                                    bias=False), nn.BatchNorm2d(self.F1 * self.D, momentum=0.01, affine=True, eps=1e-3),\n",
    "                nn.ELU(), nn.AvgPool2d((1, 4), stride=4), nn.Dropout(p=dropout))\n",
    "\n",
    "            self.block2 = nn.Sequential(\n",
    "                nn.Conv2d(self.F1 * self.D,\n",
    "                        self.F1 * self.D, (1, self.kernel_2),\n",
    "                        stride=1,\n",
    "                        padding=(0, self.kernel_2 // 2),\n",
    "                        bias=False,\n",
    "                        groups=self.F1 * self.D),\n",
    "                nn.Conv2d(self.F1 * self.D, self.F2, 1, padding=(0, 0), groups=1, bias=False, stride=1),\n",
    "                nn.BatchNorm2d(self.F2, momentum=0.01, affine=True, eps=1e-3), nn.ELU(), nn.AvgPool2d((1, 8), stride=8),\n",
    "                nn.Dropout(p=dropout))\n",
    "\n",
    "            self.lin = nn.Linear(self.feature_dim(), num_extracted_features, bias=False)\n",
    "\n",
    "\n",
    "        def feature_dim(self):\n",
    "            # function to calculate the number of features after the convolutional blocks\n",
    "            with torch.no_grad():\n",
    "                mock_eeg = torch.zeros(1, 1, self.num_electrodes, self.chunk_size)\n",
    "\n",
    "                mock_eeg = self.block1(mock_eeg)\n",
    "                mock_eeg = self.block2(mock_eeg)\n",
    "\n",
    "            return self.F2 * mock_eeg.shape[3]\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            x = x.flatten(start_dim=1)\n",
    "            x = self.lin(x)\n",
    "            return x\n",
    "    \n",
    "    class ShallowNet(nn.Module):\n",
    "        \"\"\"\n",
    "        Pytorch implementation of the ShallowNet Encoder.\n",
    "        Code taken and adjusted from:\n",
    "        https://github.com/MedMaxLab/selfEEG/blob/024402ba4bde95051d86ab2524cc71105bfd5c25/selfeeg/models/zoo.py#L693\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self,\n",
    "                    samples=1244,\n",
    "                    chans=26, # number of EEG channels\n",
    "                    F=40, # number of output filters in the temporal convolution layer\n",
    "                    K1=25, # length of the temporal convolutional layer\n",
    "                    pool=75, # temporal pooling kernel size\n",
    "                    dropout=0.2, # dropout probability\n",
    "                    num_extracted_features=num_extracted_features # number of features to extract\n",
    "                    ):\n",
    "\n",
    "            super(ShallowNet, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, F, (1, K1), stride=(1, 1))\n",
    "            self.conv2 = nn.Conv2d(F, F, (chans, 1), stride=(1, 1))\n",
    "            self.batch1 = nn.BatchNorm2d(F)\n",
    "            self.pool2 = nn.AvgPool2d((1, pool), stride=(1, 15))\n",
    "            self.flatten2 = nn.Flatten()\n",
    "            self.drop1 = nn.Dropout(dropout)\n",
    "            self.lin = nn.Linear(\n",
    "                F * ((samples - K1 + 1 - pool) // 15 + 1), num_extracted_features\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.batch1(x)\n",
    "            x = torch.square(x)\n",
    "            x = self.pool2(x)\n",
    "            x = torch.log(torch.clamp(x, 1e-7, 10000))\n",
    "            x = self.flatten2(x)\n",
    "            x = self.drop1(x)\n",
    "            x = self.lin(x)\n",
    "\n",
    "            return x\n",
    "    \n",
    "    if pretext_model == 'EEGNet':\n",
    "        pretext_model = EEGNet()\n",
    "    if pretext_model == 'ShallowNet':\n",
    "        pretext_model = ShallowNet()\n",
    "        \n",
    "    pretrained_weights = torch.load(f'{weights_dir}\\{pretext_task}_weights.pt')\n",
    "    pretrained_model = transfer_weights(pretrained_weights, pretext_model)\n",
    "    features_df = extract_features(pretrained_model, data, pretext_task, df_sample=df_sample, to_disk=to_disk, num_extracted_features=num_extracted_features)\n",
    "    if eval:\n",
    "        evaluate_features(features_df)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Across Subject pretext task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_df.shape = (1440, 101)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.190171</td>\n",
       "      <td>-19.356483</td>\n",
       "      <td>7.035101</td>\n",
       "      <td>-5.932686</td>\n",
       "      <td>1.865819</td>\n",
       "      <td>0.201499</td>\n",
       "      <td>22.717876</td>\n",
       "      <td>-1.551631</td>\n",
       "      <td>6.496997</td>\n",
       "      <td>-29.095190</td>\n",
       "      <td>...</td>\n",
       "      <td>5.371091</td>\n",
       "      <td>6.682823</td>\n",
       "      <td>14.467761</td>\n",
       "      <td>-4.801239</td>\n",
       "      <td>10.828082</td>\n",
       "      <td>4.138763</td>\n",
       "      <td>-21.877794</td>\n",
       "      <td>-3.519001</td>\n",
       "      <td>-18.737434</td>\n",
       "      <td>sub-19681349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-13.781171</td>\n",
       "      <td>-5.390237</td>\n",
       "      <td>-1.495060</td>\n",
       "      <td>-17.824326</td>\n",
       "      <td>-2.177072</td>\n",
       "      <td>4.026847</td>\n",
       "      <td>23.834311</td>\n",
       "      <td>-5.461724</td>\n",
       "      <td>-3.255945</td>\n",
       "      <td>-23.361437</td>\n",
       "      <td>...</td>\n",
       "      <td>10.863097</td>\n",
       "      <td>-2.749647</td>\n",
       "      <td>7.217631</td>\n",
       "      <td>-6.415405</td>\n",
       "      <td>5.745875</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>-17.799702</td>\n",
       "      <td>-6.542890</td>\n",
       "      <td>-14.986298</td>\n",
       "      <td>sub-19681349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-18.367344</td>\n",
       "      <td>0.138474</td>\n",
       "      <td>-2.984107</td>\n",
       "      <td>-7.175438</td>\n",
       "      <td>-0.735494</td>\n",
       "      <td>3.747396</td>\n",
       "      <td>13.888578</td>\n",
       "      <td>-4.137311</td>\n",
       "      <td>-4.859614</td>\n",
       "      <td>-28.061474</td>\n",
       "      <td>...</td>\n",
       "      <td>12.408052</td>\n",
       "      <td>-5.423985</td>\n",
       "      <td>11.853927</td>\n",
       "      <td>2.480663</td>\n",
       "      <td>7.429001</td>\n",
       "      <td>-6.486440</td>\n",
       "      <td>-18.050285</td>\n",
       "      <td>-3.872681</td>\n",
       "      <td>-8.611533</td>\n",
       "      <td>sub-19681349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1         2          3         4         5          6  \\\n",
       "0  -7.190171 -19.356483  7.035101  -5.932686  1.865819  0.201499  22.717876   \n",
       "1 -13.781171  -5.390237 -1.495060 -17.824326 -2.177072  4.026847  23.834311   \n",
       "2 -18.367344   0.138474 -2.984107  -7.175438 -0.735494  3.747396  13.888578   \n",
       "\n",
       "          7         8          9  ...         91        92         93  \\\n",
       "0 -1.551631  6.496997 -29.095190  ...   5.371091  6.682823  14.467761   \n",
       "1 -5.461724 -3.255945 -23.361437  ...  10.863097 -2.749647   7.217631   \n",
       "2 -4.137311 -4.859614 -28.061474  ...  12.408052 -5.423985  11.853927   \n",
       "\n",
       "         94         95        96         97        98         99            ID  \n",
       "0 -4.801239  10.828082  4.138763 -21.877794 -3.519001 -18.737434  sub-19681349  \n",
       "1 -6.415405   5.745875  0.000694 -17.799702 -6.542890 -14.986298  sub-19681349  \n",
       "2  2.480663   7.429001 -6.486440 -18.050285 -3.872681  -8.611533  sub-19681349  \n",
       "\n",
       "[3 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# overtrained model\n",
    "get_ssl_features('fullytrained_acrossSub_ShallowNet_pretext_model', dataset, df_replication, eval=False, to_disk=True, pretext_model='ShallowNet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
